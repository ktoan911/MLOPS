{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydXl0YUqS384"
   },
   "source": [
    "## Multi-Process One-Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ZQ7oK9MTGrw"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiprocessing ch·∫°y paralell k·∫øt h·ª£p concurency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGOGU7vVTJAf",
    "outputId": "5e20d79c-4bc5-41ca-b1d3-65a9d7db2577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpu cores:  20\n"
     ]
    }
   ],
   "source": [
    "# Check number of available cores\n",
    "print(\"Number of cpu cores: \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R-pqLdzOTWz3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep, time\n",
    "\n",
    "# Example of a processing function\n",
    "def process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n",
    "    print(f\"Processing chunk {chunk_id}\")\n",
    "    sleep(5)\n",
    "    print(f\"The chunk {chunk_id} has been processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hj8DQqxb7NqN",
    "outputId": "e52c1d7b-c491-4227-9c9f-9d9059856a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 24243\n",
      "Processing chunk 0\n",
      "Process ID: 24248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Process ID: 24251\n",
      "The chunk 0 has been processed successfully!\n",
      "The chunk 1 has been processed successfully!\n",
      "The chunk 2 has been processed successfully!\n",
      "Total time: 5.08407735824585s\n"
     ]
    }
   ],
   "source": [
    "# Make a sample dataframe\n",
    "data = [\n",
    "    ['tom', 10], ['nick', 15],\n",
    "    ['juli', 14], ['peter', 20],\n",
    "    ['jason', 27], ['anna', 11]\n",
    "]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['Name', 'Age'])\n",
    "\n",
    "# Devide the dataframe into chunks\n",
    "chunk_size = 2\n",
    "chunks = [df[i:i+chunk_size].to_numpy() for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Mark the starting point to measure processing time\n",
    "start = time()\n",
    "\n",
    "procs = []\n",
    "#l·∫∑p qua c√°c chunk, m·ªói chunk ƒë∆∞a v√†o 1 proc\n",
    "# kh√¥ng ƒë·ªìng ƒë·ªÅu do ƒë√¥i khi corelogic ƒëang b·∫≠n x·ª≠ l√Ω c√°i kh√°c ch∆∞a x·ª≠ l√Ω k·ªãp h√†m process_dataframe\n",
    "# tuy nhi√™n M·ªói l·∫ßn Process(...) ƒë∆∞·ª£c t·∫°o v√† start(), h·ªá ƒëi·ªÅu h√†nh s·∫Ω g√°n ti·∫øn tr√¨nh ƒë√≥ v√†o m·ªôt logic core c·ª• th·ªÉ \n",
    "# (ho·∫∑c lu√¢n chuy·ªÉn gi·ªØa c√°c core n·∫øu c·∫ßn), nh∆∞ng vi·ªác g√°n v√†o core n√†o l√† do h·ªá ƒëi·ªÅu h√†nh quy·∫øt ƒë·ªãnh.\n",
    "for i, chunk in enumerate(chunks):\n",
    "  # Define our process but not yet start\n",
    "  proc = Process(target=process_dataframe, args=(i, chunk,))\n",
    "  # Start the process\n",
    "  proc.start()\n",
    "  # Investigate process ID\n",
    "  print(f\"Process ID: {proc.pid}\")\n",
    "  # Manage all process definitions in a list\n",
    "  procs.append(proc)\n",
    "\n",
    "# Stop all processes to prevent resource scarcity\n",
    "# imagine zombie processes\n",
    "# ch·ªù c√°c proc x·ª≠ l√Ω xong h·∫øt ƒë·ªÉ tr√°nh t√¨nh tr·∫°ng zombie proc\n",
    "for proc in procs:\n",
    "    proc.join() # Wait for the process to finish\n",
    "\n",
    "# Report total elapsed time\n",
    "# n·∫øu kh√¥ng c√≥ join ph√≠a tr√™n ƒë√¥i khi d√≤ng d∆∞·ªõi c√≤n ch·∫°y xong tr∆∞·ªõc\n",
    "print(f\"Total time: {time() - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dI1nvsCSOzYM",
    "outputId": "0374964f-9a77-4f05-f025-f3d97a4ddfc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0Processing chunk 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chunk 1 has been processed successfully!The chunk 0 has been processed successfully!\n",
      "\n",
      "Processing chunk 2\n",
      "The chunk 2 has been processed successfully!\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# The 2nd way to do multiprocess is via Pool #\n",
    "##############################################\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Define a pool of processes\n",
    "NUM_PROCESSES = 2 # c√≥ 2 proc ƒëc t·∫°o ra\n",
    "pool = Pool(NUM_PROCESSES)\n",
    "\n",
    "procs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    procs.append(\n",
    "        # The main process does not need to wait for this function\n",
    "        pool.apply_async(process_dataframe, args=(i, chunk))\n",
    "    )\n",
    "\n",
    "for proc in procs:\n",
    "    proc.get() # Wait for the process to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hNl_vM4hWJUj"
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# The 3rd way to do multiprocess is via Map #\n",
    "##############################################\n",
    "inputs = [(i, chunk) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# If you have one arg only, please use `.map` instead\n",
    "outputs = pool.starmap(\n",
    "    process_dataframe,\n",
    "    inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICbCCRWkUKHW",
    "outputId": "a35359df-998b-40ff-8403-453ea3309eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n",
      "The chunk 0 has been processed successfully!\n",
      "Processing chunk 1\n",
      "The chunk 1 has been processed successfully!\n",
      "Processing chunk 2\n",
      "The chunk 2 has been processed successfully!\n",
      "Total time: 15.079243898391724s\n"
     ]
    }
   ],
   "source": [
    "# Uhm... you can see that sometimes, two strings are concatnated w/o any `/n` ( race condition problem)\n",
    "# this is because all the processes writes to stdout at the same time, we need\n",
    "# to find a way to prevent other processes write to it while one is doing\n",
    "from multiprocessing import Lock\n",
    "\n",
    "lock = Lock()\n",
    "\n",
    "def process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n",
    "    lock.acquire() #process ƒëang x√©t ƒëang c√≥ quy·ªÅn c·∫ßm kh√≥a\n",
    "    # do ƒëang c·∫ßm kh√≥a n√™n ƒë√¢y l√† th·∫±ng duy nh·∫•t ƒë∆∞·ª£c ghi v√†o terminal\n",
    "    #M·ªói khi b·∫°n d√πng lock.acquire(), b·∫°n ƒëang n√≥i v·ªõi ch∆∞∆°ng tr√¨nh r·∫±ng ‚Äúƒëo·∫°n m√£ ti·∫øp theo c·∫ßn ƒë∆∞·ª£c th·ª±c hi·ªán theo \n",
    "    # c√°ch an to√†n, ch·ªâ c√≥ m·ªôt lu·ªìng duy nh·∫•t ƒë∆∞·ª£c ph√©p thao t√°c v√†o t√†i nguy√™n n√†y t·∫°i m·ªôt th·ªùi ƒëi·ªÉm‚Äù.\n",
    "    print(f\"Processing chunk {chunk_id}\")\n",
    "    sleep(5)\n",
    "    print(f\"The chunk {chunk_id} has been processed successfully!\")\n",
    "    lock.release() #process ƒëang x√©t nh·∫£ kh√≥a ra\n",
    "\n",
    "# Make a sample dataframe\n",
    "data = [\n",
    "    ['tom', 10], ['nick', 15],\n",
    "    ['juli', 14], ['peter', 20],\n",
    "    ['jason', 27], ['anna', 11]\n",
    "]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['Name', 'Age'])\n",
    "\n",
    "# Devide the dataframe into chunks\n",
    "chunk_size = 2\n",
    "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Mark the starting point to measure processing time\n",
    "start = time()\n",
    "\n",
    "procs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "  # Define our process but not yet start\n",
    "  proc = Process(target=process_dataframe, args=(i, chunk,))\n",
    "  # Start the process\n",
    "  proc.start()\n",
    "  # Manage all process definitions in a list\n",
    "  procs.append(proc)\n",
    "\n",
    "# Stop all processes to prevent resource scarcity\n",
    "# imagine zombie processes\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "\n",
    "# Report total elapsed time\n",
    "print(f\"Total time: {time() - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax3LFgHcS93u"
   },
   "source": [
    "## Multi-Threaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt-98oQ_CRgG"
   },
   "source": [
    "Python provides the same APIs to multiprocessing with `start()` and `join()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thread c√πng chia s·∫ª v√πng nh·ªõ v·ªõi nhau khi n√≥ c√πng trong process ( proc c√≥ th·ªÉ t·∫°o nhi·ªÅu thread) \n",
    "GIL ( Khi c√≥ 1 proc c√≥ kh·∫£ nƒÉng excute nhi·ªÅu task, m·ªói task s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi c√°c thread -> x·∫£y ra t√¨nh tr·∫°ng thread x·ª≠ l√Ω task n√†y nh·∫£y sang task kh√°c -> c√πng c·∫≠p nh·∫≠t variable X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† GIL (Global Interpreter Lock) l√† g√¨?\n",
    "\n",
    "GIL l√† m·ªôt c∆° ch·∫ø trong CPython (tr√¨nh th√¥ng d·ªãch Python ph·ªï bi·∫øn nh·∫•t) ƒë·∫£m b·∫£o r·∫±ng ch·ªâ m·ªôt thread Python ƒë∆∞·ª£c th·ª±c thi t·∫°i m·ªôt th·ªùi ƒëi·ªÉm, ngay c·∫£ khi b·∫°n s·ª≠ d·ª•ng nhi·ªÅu thread.\n",
    "\n",
    "ü§î T·∫°i sao l·∫°i c√≥ GIL?\n",
    "\n",
    "CPython kh√¥ng thread-safe khi truy c·∫≠p v√†o c√°c ƒë·ªëi t∆∞·ª£ng Python trong b·ªô nh·ªõ.\n",
    "\n",
    "GIL gi√∫p ƒë∆°n gi·∫£n h√≥a vi·ªác qu·∫£n l√Ω b·ªô nh·ªõ v√† garbage collection.\n",
    "\n",
    "Nh∆∞ng ƒë·ªïi l·∫°i, n√≥ g√¢y ra h·∫°n ch·∫ø v·ªõi c√°c ch∆∞∆°ng tr√¨nh ƒëa lu·ªìng (multi-threading), ƒë·∫∑c bi·ªát khi x·ª≠ l√Ω t√°c v·ª• CPU-bound.\n",
    "\n",
    "üìå Khi n√†o GIL kh√¥ng ph·∫£i l√† v·∫•n ƒë·ªÅ?\n",
    "\n",
    "V·ªõi I/O-bound tasks (nh∆∞ ƒë·ªçc ghi file, g·ªçi API, ch·ªù m·∫°ng...), GIL kh√¥ng ·∫£nh h∆∞·ªüng nhi·ªÅu.\n",
    "\n",
    "V·ªõi CPU-bound tasks (t√≠nh to√°n n·∫∑ng), GIL l√†m multi-threading k√©m hi·ªáu qu·∫£. L√∫c n√†y, n√™n d√πng multiprocessing (ƒëa ti·∫øn tr√¨nh) thay v√¨ ƒëa lu·ªìng.\n",
    "\n",
    "üì¶ M·ªôt s·ªë c√°ch ‚Äúv∆∞·ª£t qua‚Äù GIL:\n",
    "\n",
    "S·ª≠ d·ª•ng multiprocessing thay v√¨ threading\n",
    "\n",
    "D√πng th∆∞ vi·ªán C m·ªü r·ªông (nh∆∞ NumPy, ho·∫∑c c√°c th∆∞ vi·ªán Cython cho ph√©p th·∫£ GIL)\n",
    "\n",
    "Chuy·ªÉn sang tr√¨nh th√¥ng d·ªãch kh√¥ng c√≥ GIL, nh∆∞:\n",
    "\n",
    "PyPy (d√π hi·ªán v·∫´n c√≥ GIL)\n",
    "\n",
    "Jython (ch·∫°y tr√™n JVM, kh√¥ng c√≥ GIL)\n",
    "\n",
    "IronPython (ch·∫°y tr√™n .NET CLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QR5SBZ1poPGE",
    "outputId": "fea68c6b-174a-47f1-c482-66f016a2bc02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n",
      "The chunk 0 has been processed successfully!\n",
      "Processing chunk 1\n",
      "The chunk 1 has been processed successfully!\n",
      "Processing chunk 2\n",
      "The chunk 2 has been processed successfully!\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "my_threads = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    my_thread = Thread(target=process_dataframe, args=(i, chunk,))\n",
    "    my_thread.start()\n",
    "    my_threads.append(my_thread)\n",
    "\n",
    "for my_thread in my_threads:\n",
    "    my_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roVklnzdXyvA"
   },
   "source": [
    "However, the way they share data is different. Let's take a look at the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apL2XLwTefei",
    "outputId": "68b2ccca-03f3-4b66-82ef-da3902d91627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value of shared_data: 3\n"
     ]
    }
   ],
   "source": [
    "from threading import Lock\n",
    "\n",
    "# Define a lock to prevent race condition,\n",
    "# which is multiple updates into the same variable\n",
    "lock = Lock()\n",
    "\n",
    "# Share data\n",
    "shared_data = 0\n",
    "\n",
    "def increment_function():\n",
    "    global shared_data\n",
    "    with lock: # This is equal to acquire() + release()\n",
    "        shared_data += 1\n",
    "\n",
    "my_threads = []\n",
    "for _ in range(3):\n",
    "    my_thread = Thread(target=increment_function)\n",
    "    my_thread.start()\n",
    "    my_threads.append(my_thread)\n",
    "\n",
    "for my_thread in my_threads:\n",
    "    my_thread.join()\n",
    "\n",
    "print(f\"Current value of shared_data: {shared_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zlNXu-lEpUR",
    "outputId": "7164ee3b-5bd6-42bd-dc97-a652784f854c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value of shared_data: 3\n"
     ]
    }
   ],
   "source": [
    "# Another way to access shared_data is via Queue\n",
    "from queue import Queue\n",
    "\n",
    "# Initialize the queue\n",
    "shared_queue = Queue()\n",
    "\n",
    "def increment_function():\n",
    "    shared_queue.put(1)\n",
    "\n",
    "my_threads = []\n",
    "for _ in range(3):\n",
    "    my_thread = Thread(target=increment_function)\n",
    "    my_thread.start()\n",
    "    my_threads.append(my_thread)\n",
    "\n",
    "for i, my_thread in enumerate(my_threads):\n",
    "    my_thread.join()\n",
    "\n",
    "shared_data = 0\n",
    "while not shared_queue.empty():\n",
    "    shared_data += shared_queue.get()\n",
    "\n",
    "print(f\"Current value of shared_data: {shared_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQYIIevMMqpS",
    "outputId": "d6a0cab0-ebb0-4ecf-882a-ada0b071b0ee"
   },
   "outputs": [],
   "source": [
    "# Using a global variable is not a piece of cake in multiprocessing\n",
    "# as in multithreading\n",
    "import multiprocessing\n",
    "\n",
    "# Define a shared value between oprocesses\n",
    "shared_variable = multiprocessing.Value('i', 0)\n",
    "\n",
    "def worker_function():\n",
    "    global shared_variable\n",
    "    with shared_variable.get_lock():\n",
    "        shared_variable.value += 1\n",
    "\n",
    "procs = []\n",
    "for _ in range(3):\n",
    "  # Define our process but not yet start\n",
    "  proc = Process(target=worker_function)\n",
    "  # Start the process\n",
    "  proc.start()\n",
    "  # Manage all process definitions in a list\n",
    "  procs.append(proc)\n",
    "\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "\n",
    "print(f\"Current value of shared_data: {shared_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtAeq1cUQJ9E",
    "outputId": "1fac16a4-7a16-4a9d-ad7d-ab72a0ae1d1d"
   },
   "outputs": [],
   "source": [
    "# Or using queue\n",
    "from multiprocessing import Queue\n",
    "from multiprocessing import Process\n",
    "\n",
    "def worker_function(shared_queue):\n",
    "    shared_queue.put(1)\n",
    "\n",
    "shared_queue = Queue()\n",
    "\n",
    "procs = []\n",
    "for _ in range(3):\n",
    "    my_proc = Process(target=worker_function, args=(shared_queue,))\n",
    "    my_proc.start()\n",
    "    procs.append(my_proc)\n",
    "\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "\n",
    "# The main process retrieves data from the queue\n",
    "# and aggregate the result\n",
    "result = 0\n",
    "while not shared_queue.empty():\n",
    "    result += shared_queue.get(1)\n",
    "\n",
    "print(f\"Results from multiprocessing queue: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVcMwStgTBIo"
   },
   "source": [
    "## One-Process One-Thread\n",
    "### V√†o event_loop_demo ƒë·ªÉ test k·ªπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfb2IrxEIMW7",
    "outputId": "3c554bd6-c8a1-42f8-9ca5-7de11d7eb3f4"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     loop.close()\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Run the event loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# YOU WILL MEET SOME WEIRD ERRORS HERE DUE TO IPYTHON NOTEBOOK,\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# LET'S MOVE TO A PYTHON SCRIPT\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m     17\u001b[39m     loop = asyncio.get_event_loop() \u001b[38;5;66;03m# n∆°i ch·ª©a task 1 thread c·∫ßn th·ª±c hi·ªán\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_my_1st_data_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_my_2nd_data_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     loop.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/base_events.py:663\u001b[39m, in \u001b[36mBaseEventLoop.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    652\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[32m    653\u001b[39m \n\u001b[32m    654\u001b[39m \u001b[33;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    660\u001b[39m \u001b[33;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m new_task = \u001b[38;5;129;01mnot\u001b[39;00m futures.isfuture(future)\n\u001b[32m    666\u001b[39m future = tasks.ensure_future(future, loop=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/base_events.py:622\u001b[39m, in \u001b[36mBaseEventLoop._check_running\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_running():\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThis event loop is already running\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    624\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    625\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mCannot run the event loop while another loop is running\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: This event loop is already running"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting my 1st data func...\n",
      "Starting my 2nd data func...\n",
      "Completed my 1st data func!\n",
      "Completed my 2nd data func!\n"
     ]
    }
   ],
   "source": [
    "# Let's say you have one process with one thread only, how to deal with it?\n",
    "# AsyncIO helps us to do it\n",
    "#NOTE: CH·ªà N√äN D√ôNG AWAIT CHO C√ÅC T√ÅC V·ª§ I/O\n",
    "import asyncio\n",
    "\n",
    "#coroutine\n",
    "async def download_my_1st_data_func():\n",
    "    print(\"Starting my 1st data func...\")\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Completed my 1st data func!\")\n",
    "\n",
    "async def download_my_2nd_data_func():\n",
    "    print(\"Starting my 2nd data func...\")\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Completed my 2nd data func!\")\n",
    "\n",
    "def main():\n",
    "    loop = asyncio.get_event_loop() # n∆°i ch·ª©a task 1 thread c·∫ßn th·ª±c hi·ªán\n",
    "    loop.run_until_complete(asyncio.gather(\n",
    "        download_my_1st_data_func(),\n",
    "        download_my_2nd_data_func(),\n",
    "    ))\n",
    "    loop.close()\n",
    "\n",
    "# Run the event loop\n",
    "main()\n",
    "\n",
    "# YOU WILL MEET SOME WEIRD ERRORS HERE DUE TO IPYTHON NOTEBOOK,\n",
    "# LET'S MOVE TO A PYTHON SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
